# -*- coding: utf-8 -*-
"""Copy of SCOTUS_WebScrapper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RqLM2-3jOHqT8XVo5dji8hTvv20xqhRu
"""

!pip install beautifulsoup4  
!pip install pdfminer.six

from http.client import responses
from logging import exception
from requests.models import HTTPError
import re
import requests
from bs4 import BeautifulSoup
import threading
import time
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer, LTChar,LTLine,LAParams
import os
import re
from concurrent.futures import ThreadPoolExecutor
import csv

global lock
lock = threading.Semaphore(6)

"""PDFMINING.py

"""

def process_document(title,response):
    #lock.acquire()
    try:
      counter = 0
      file = open(str(counter)+".pdf", 'wb').write(response.content)
      result_entry = title +","+ str(extract_result(str(counter)+".pdf"))
    except:
      result_entry = title +","+"[4]"
    print(result_entry)
    #scotus_writer.writerow(list(result_entry))
    #lock.release()
    output = open("results.txt", "a")  # append mode
    output.write(result_entry+"\n")
    output.close()
    
    #uncomment the line above to store all files

def extract_result(path):
    
    
    result_options = [r"affirmed",r"remanded",r"vacate",r"reversed"]
    
    prefix = r"[0-9]+ F. [0-9]{0,1}d [0-9]+,(.*)"
    whole = r"[0-9]+ F. [0-9]{0,1}d [0-9]+,(.*)\.( )*$"
    suffix = r"^(.*)\.(.*)$"
    end_of_syllabus = r"Opinion(.*)\."

    case = ""
    results = []
    # algorithm looks for verdict
    #pattern matching by docket number:
    lastline = ""

    line_log = []
    index = 0
    for layout in extract_pages(path):
        for element in layout:
            if isinstance(element, LTTextContainer):
                for line in element:
                    for character in line:
                        if isinstance(character, LTChar):
                            font_size = character.size
                line_log.append((font_size,index,element.get_text()))
                index += 1
    second_header = [li for li in line_log if li[0] == 15.0][1]
    i = 0
    while i < (len(line_log)-len(line_log[second_header[1]])):
        #pattern matching
        current_line = line_log[second_header[1]-i]
        prefixmatches = re.search(prefix,str(current_line)) is not None
        #llprefixmatches = re.search(prefix,lastline) is not None
        suffixmatches = re.search(suffix,str(current_line)) is not None
        for word in result_options:
            matches = re.search(word,str(current_line)) is not None
            if(prefixmatches and matches and suffixmatches):
                results.append(result_options.index(word))
            
        i += 1
    
    if(len(results) == 0): results = [4]
    return list(set(results))

"""OpinionScraper.py

"""

def scrape_opinions():
    open("results.txt","w").close()
    # sample headers from the internet
    headers = {
    'Access-Control-Allow-Origin': '*',
    'Access-Control-Allow-Methods': 'GET',
    'Access-Control-Allow-Headers': 'Content-Type',
    'Access-Control-Max-Age': '3600',
    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'
    }
    
    with ThreadPoolExecutor(max_workers=6) as e:

        for page in range(21,19,-1):
            response = requests.get("https://www.supremecourt.gov/opinions/slipopinion/"+str(page).zfill(2)) # zfill so if it is 5, it becomes 05, zfill of 2 makes sure length is 2.
            
            soup = BeautifulSoup(response.content,"html.parser")
            #table = soup.find("table",{"class":""})
            
            for anchor_tag in soup.findAll('a'):
                isopinion = re.search(r"/opinions/(.*).pdf$",str(anchor_tag.get('href'))) is not None
                proper_title = re.search(r"v.",str(anchor_tag.text)) is not None
                if(proper_title and isopinion):
                    
                    link = "https://www.supremecourt.gov" + str(anchor_tag.get('href'))
                    
                    
                    try:
                        time.sleep(0.3)
                        response = requests.get(link)
                        title = anchor_tag.text
                        #e.submit(process_document,title,response,output)
                        process_document(title,response)
                    except UnicodeError:
                        print("Unicode Error")
                    except IndexError:
                        print("Index Error")
                    except:
                        print("Other Error")

def Convert(string):
    li = list(string)
    return li

if __name__ == "__main__":
  with open('scotus_headers.csv', mode='w') as scotus_csv:
      scotus_writer = csv.writer(scotus_csv, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
  scrape_opinions()

#from google.colab import drive
#drive.mount('/content/drive')